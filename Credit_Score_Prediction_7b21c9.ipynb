{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 3846912,
          "sourceType": "datasetVersion",
          "datasetId": 2289007
        }
      ],
      "dockerImageVersionId": 30558,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Credit Score Prediction   7b21c9",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamuelOnyangoOmondi/alu-machine_learning/blob/main/Credit_Score_Prediction_7b21c9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'credit-score-classification:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F2289007%2F3846912%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240228%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240228T091322Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D4d44f033c0c9ba0151c8ca9765724c2c5e4b31989b593883a3a16601518dc3a82a14cc6806a63bb8c3f5284081848ba594d02e3aaebba92b8aa1f5044cf15b33763674020a1f3e978e7babdf640af2c192470b18bb03ed7b75cdb34638aa4f02b3037ee558984afc8d53c106911f0f49b2fe3c296aa1764237e98ea0b6ff0e1bfdb238c7a81fa26dff12bc73d3bf27419cffbcee1e64d35d04bc7279507456f49121a2632a41949eaaf01da8ffa625239413a4be04cae6b8b0fb28cb13e76ed66fa87a8f31f765462c4158b234ffbe2a5d9d9e3c9d34cd142b3a17a7b066309305675990667ed86b36b04b02cabd2d87fae10652c40cc8433e8d7b8d01544f77'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "Io1gGte8_Xql",
        "outputId": "015d2dbd-deda-4742-c57f-ade76c5231db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading credit-score-classification, 9973595 bytes compressed\n",
            "[==================================================] 9973595 bytes downloaded\n",
            "Downloaded and uncompressed: credit-score-classification\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting Credit Scores\n",
        "Welcome to this data science project focused on predicting credit scores in the banking industry. In this notebook, we embark on a mission to harness the power of data and machine learning to address a crucial aspect of financial decision-making.\n",
        "\n",
        "### Mission Objective:\n",
        "Our primary goal is to develop a robust and accurate predictive model for credit scores. Credit scores play a pivotal role in the world of finance, influencing lending decisions, interest rates, and access to various financial services. By predicting credit scores effectively, we aim to:\n",
        "\n",
        "* Empower Informed Decisions: Provide financial institutions with a tool to make informed decisions regarding loan approvals, interest rates, and credit limits, ultimately improving risk management.\n",
        "\n",
        "* Enhance Financial Inclusion: Enable fairer access to credit by identifying individuals who may be creditworthy but are often underserved by traditional credit scoring methods.\n",
        "\n",
        "* Minimize Defaults: Reduce the risk of defaults by identifying high-risk applicants early in the lending process."
      ],
      "metadata": {
        "id": "QpwBta8Y_Xqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATA OVERVIEW\n",
        "\n",
        "'id': A unique identifier for each data record.\n",
        "\n",
        "'customer_id': An identifier for each customer, allowing you to associate multiple records with the same individual.\n",
        "\n",
        "'month': The month of the data record, indicating when the data was collected or relevant.\n",
        "\n",
        "'name': The name of the customer, which may be used for identification purposes.\n",
        "\n",
        "'age': The age of the customer, providing information about their age demographic.\n",
        "\n",
        "'ssn': The Social Security Number (SSN) of the customer, a unique identifier used for verification.\n",
        "\n",
        "'occupation': The occupation or profession of the customer, which can help understand their employment status.\n",
        "\n",
        "'annual_income': The annual income of the customer, a crucial financial parameter.\n",
        "\n",
        "'monthly_inhand_salary': The monthly salary or income available to the customer after deductions.\n",
        "\n",
        "'num_bank_accounts': The number of bank accounts held by the customer, indicating their banking activity.\n",
        "\n",
        "'num_credit_card': The number of credit cards held by the customer, reflecting their credit usage.\n",
        "\n",
        "'interest_rate': The interest rate associated with the customer's financial products, such as loans or credit cards.\n",
        "\n",
        "'num_of_loan': The number of loans the customer has, providing insight into their debt obligations.\n",
        "\n",
        "'type_of_loan': The types of loans the customer holds, which can include mortgages, personal loans, etc.\n",
        "\n",
        "'delay_from_due_date': The delay in payment from the due date for loans or credit cards, indicating their payment behavior.\n",
        "\n",
        "'num_of_delayed_payment': The number of delayed payments made by the customer.\n",
        "\n",
        "'changed_credit_limit': Changes in the customer's credit limit, which can affect their credit utilization.\n",
        "\n",
        "'num_credit_inquiries': The number of credit inquiries made by the customer, potentially affecting their credit score.\n",
        "\n",
        "'credit_mix': The composition of the customer's credit accounts, which can impact their credit profile.\n",
        "\n",
        "'outstanding_debt': The amount of outstanding debt owed by the customer.\n",
        "\n",
        "'credit_utilization_ratio': The ratio of credit used to the total available credit, a key factor in credit scoring.\n",
        "\n",
        "'credit_history_age': The age of the customer's credit history, influencing their creditworthiness.\n",
        "\n",
        "'payment_of_min_amount': How customers handle the minimum payment amount on credit cards or loans.\n",
        "\n",
        "'total_emi_per_month': The total Equated Monthly Installment (EMI) payments made by the customer.\n",
        "\n",
        "'amount_invested_monthly': The amount the customer invests on a monthly basis, if applicable.\n",
        "\n",
        "'payment_behaviour': The behavior of the customer regarding their payments, reflecting their financial responsibility.\n",
        "\n",
        "'monthly_balance': The monthly balance in the customer's financial accounts.\n",
        "\n",
        "'credit_score': The target variable representing the customer's credit score, which we aim to predict."
      ],
      "metadata": {
        "id": "YNcuGhhG_Xqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:24:25.786561Z",
          "iopub.execute_input": "2023-10-06T17:24:25.786949Z",
          "iopub.status.idle": "2023-10-06T17:24:26.226981Z",
          "shell.execute_reply.started": "2023-10-06T17:24:25.786919Z",
          "shell.execute_reply": "2023-10-06T17:24:26.225741Z"
        },
        "trusted": true,
        "id": "aCCDo2R9_Xqr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/kaggle/input/credit-score-classification/train.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:24:26.572005Z",
          "iopub.execute_input": "2023-10-06T17:24:26.573314Z",
          "iopub.status.idle": "2023-10-06T17:24:27.924719Z",
          "shell.execute_reply.started": "2023-10-06T17:24:26.573281Z",
          "shell.execute_reply": "2023-10-06T17:24:27.923504Z"
        },
        "trusted": true,
        "id": "6CmywPjb_Xqr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = data.copy()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:24:29.212025Z",
          "iopub.execute_input": "2023-10-06T17:24:29.213238Z",
          "iopub.status.idle": "2023-10-06T17:24:29.23221Z",
          "shell.execute_reply.started": "2023-10-06T17:24:29.213201Z",
          "shell.execute_reply": "2023-10-06T17:24:29.231373Z"
        },
        "trusted": true,
        "id": "rb12c-Yk_Xqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = df.columns.str.lower()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:24:29.956372Z",
          "iopub.execute_input": "2023-10-06T17:24:29.957663Z",
          "iopub.status.idle": "2023-10-06T17:24:29.964039Z",
          "shell.execute_reply.started": "2023-10-06T17:24:29.957592Z",
          "shell.execute_reply": "2023-10-06T17:24:29.963046Z"
        },
        "trusted": true,
        "id": "3KhZrFUw_Xqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:24:30.708457Z",
          "iopub.execute_input": "2023-10-06T17:24:30.709234Z",
          "iopub.status.idle": "2023-10-06T17:24:30.719723Z",
          "shell.execute_reply.started": "2023-10-06T17:24:30.709168Z",
          "shell.execute_reply": "2023-10-06T17:24:30.718624Z"
        },
        "trusted": true,
        "id": "hiEuN6Il_Xqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:24:31.436055Z",
          "iopub.execute_input": "2023-10-06T17:24:31.436621Z",
          "iopub.status.idle": "2023-10-06T17:24:31.445853Z",
          "shell.execute_reply.started": "2023-10-06T17:24:31.436577Z",
          "shell.execute_reply": "2023-10-06T17:24:31.444905Z"
        },
        "trusted": true,
        "id": "rgLDUOPN_Xqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:24:32.069018Z",
          "iopub.execute_input": "2023-10-06T17:24:32.069778Z",
          "iopub.status.idle": "2023-10-06T17:24:32.083553Z",
          "shell.execute_reply.started": "2023-10-06T17:24:32.069737Z",
          "shell.execute_reply": "2023-10-06T17:24:32.082511Z"
        },
        "trusted": true,
        "id": "5K_E3LmZ_Xqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "df.sample(5)"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-10-06T17:24:32.796532Z",
          "iopub.execute_input": "2023-10-06T17:24:32.797154Z",
          "iopub.status.idle": "2023-10-06T17:24:32.839251Z",
          "shell.execute_reply.started": "2023-10-06T17:24:32.797091Z",
          "shell.execute_reply": "2023-10-06T17:24:32.838192Z"
        },
        "trusted": true,
        "id": "R1L7g-TS_Xqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-10-06T17:24:33.572504Z",
          "iopub.execute_input": "2023-10-06T17:24:33.572856Z",
          "iopub.status.idle": "2023-10-06T17:24:33.672359Z",
          "shell.execute_reply.started": "2023-10-06T17:24:33.572831Z",
          "shell.execute_reply": "2023-10-06T17:24:33.671521Z"
        },
        "trusted": true,
        "id": "XJn_ZnQU_Xqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-10-06T17:24:34.348722Z",
          "iopub.execute_input": "2023-10-06T17:24:34.349423Z",
          "iopub.status.idle": "2023-10-06T17:24:34.407121Z",
          "shell.execute_reply.started": "2023-10-06T17:24:34.349391Z",
          "shell.execute_reply": "2023-10-06T17:24:34.406428Z"
        },
        "trusted": true,
        "id": "mEpogQjM_Xqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## INVESTIGATION COLUMN BY COLUMN"
      ],
      "metadata": {
        "id": "OiVqXCWk_Xqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "object_columns = df.columns.tolist()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:24:36.821045Z",
          "iopub.execute_input": "2023-10-06T17:24:36.821666Z",
          "iopub.status.idle": "2023-10-06T17:24:36.825785Z",
          "shell.execute_reply.started": "2023-10-06T17:24:36.821638Z",
          "shell.execute_reply": "2023-10-06T17:24:36.82478Z"
        },
        "trusted": true,
        "id": "O_UhC-pO_Xqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in object_columns:\n",
        "    print('Column Name: '+col)\n",
        "    print(\"**\"*20)\n",
        "    print(df[col].value_counts(dropna=False))\n",
        "    print('END', \"--\"*18, '\\n')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:24:38.397147Z",
          "iopub.execute_input": "2023-10-06T17:24:38.397574Z",
          "iopub.status.idle": "2023-10-06T17:24:38.678164Z",
          "shell.execute_reply.started": "2023-10-06T17:24:38.397525Z",
          "shell.execute_reply": "2023-10-06T17:24:38.677063Z"
        },
        "trusted": true,
        "id": "xvgPDqEp_Xqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Categorical Features Handling Plan\n",
        "\n",
        "\n",
        "##### month\n",
        "* int64 --> str\n",
        "* This column will be encoded\n",
        "\n",
        "##### occupation\n",
        "* `_______` values will be deleted.\n",
        "* Values will be imputed using the mode, which represents the most common value associated with each unique Customer_ID\n",
        "* This column will be encoded.\n",
        "\n",
        "#####  type_of_loan\n",
        "* Get every specific loan type.\n",
        "* Binary encoding. If a person has that specific loan type in the cell, it will be True otherwise False.\n",
        "* Implement mode imputer to missing values\n",
        "\n",
        "##### credit_mix\n",
        "* `_` values will be deleted\n",
        "* Missing values will be imputed using the mode, which represents the most common value associated with each unique Customer_ID\n",
        "* This column will be encoded.\n",
        "\n",
        "##### payment_of_min_amount\n",
        "* `NM` values will be deleted\n",
        "* Missing values will be imputed using the mode, which represents the most common value associated with each unique Customer_ID\n",
        "* This column will be encoded.\n",
        "\n",
        "##### payment_behaviour\n",
        "* `!@9#%8` values will be deleted.\n",
        "* Missing values will be imputed using the mode, which represents the most common value associated with each unique Customer_ID\n",
        "* This column will be encoded.\n",
        "\n",
        "##### credit_score\n",
        "* This column is the target. There are three values; Standard, Poor, Good. They will be replaced as Poor --> 0, Good --> 1, Standard --> 2\n",
        "\n",
        "##### name / ssn / customer_id\n",
        "* These columns will be deleted at the end of the categorical feature handling process."
      ],
      "metadata": {
        "id": "9imV_VBF_Xqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_columns = ['credit_mix',\n",
        "                      'month',\n",
        "                      'occupation',\n",
        "                      'payment_behaviour',\n",
        "                      'payment_of_min_amount',\n",
        "                      'type_of_loan',\n",
        "                      'credit_score',\n",
        "                      'id',\n",
        "                      'customer_id',\n",
        "                      'name',\n",
        "                      'ssn']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:24:41.070049Z",
          "iopub.execute_input": "2023-10-06T17:24:41.070436Z",
          "iopub.status.idle": "2023-10-06T17:24:41.07618Z",
          "shell.execute_reply.started": "2023-10-06T17:24:41.070405Z",
          "shell.execute_reply": "2023-10-06T17:24:41.074736Z"
        },
        "trusted": true,
        "id": "5Izqpyiy_Xqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CategoricalTransformer():\n",
        "    \"\"\"\n",
        "    This class provides methods for handling and transforming categorical features in a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - input_df (pd.DataFrame): The input DataFrame containing categorical features.\n",
        "\n",
        "    Methods:\n",
        "    - month_handling: Encodes and handles the 'month' column.\n",
        "    - occupation_handling: Encodes and handles the 'occupation' column.\n",
        "    - type_of_loan_handling: Handles and encodes the 'type_of_loan' column.\n",
        "    - credit_mix_handling: Encodes and handles the 'credit_mix' column.\n",
        "    - payment_of_min_amount_handling: Encodes and handles the 'payment_of_min_amount' column.\n",
        "    - payment_behaviour_handling: Encodes and handles the 'payment_behaviour' column.\n",
        "    - credit_score_handling: Encodes the 'credit_score' column.\n",
        "    - column_deleter: Deletes unnecessary columns and returns the modified DataFrame.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self,input_df:pd.DataFrame):\n",
        "        self.df = input_df\n",
        "\n",
        "    def month_handling(self) -> pd.DataFrame:\n",
        "        if 'month' not in self.df.columns:\n",
        "            raise ValueError ('\"month\" not found in the DataFrame.')\n",
        "\n",
        "\n",
        "        self.df['month'] = self.df['month'].astype(str)\n",
        "        # Encoding without first value\n",
        "        month_dummies = pd.get_dummies(self.df['month'], prefix='month', drop_first=True, dtype=int)\n",
        "\n",
        "        # Creating new dataframe with dummies (month_january) and without original column.\n",
        "        self.df = pd.concat([self.df, month_dummies], axis=1)\n",
        "        self.df.drop('month', axis=1, inplace=True)\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def occupation_handling(self) -> pd.DataFrame:\n",
        "        if 'occupation' not in self.df.columns:\n",
        "            raise ValueError ('\"occupation\" not found in the DataFrame.')\n",
        "\n",
        "        # Deleting strange values\n",
        "        self.df['occupation'] = self.df['occupation'].replace(\"_______\", pd.NA)\n",
        "\n",
        "        # Replacing missing values in each row, associated with a specific customer_id, with the most frequent value.\n",
        "        self.df['occupation'].astype(str)\n",
        "        self.df['occupation'] = self.df.groupby('customer_id')['occupation'].transform(lambda x: x.mode()[0])\n",
        "\n",
        "        # Binary encoding without first value.\n",
        "        occupation_dummies = pd.get_dummies(self.df['occupation'], drop_first=True, prefix='occupation', dtype=int)\n",
        "        self.df = pd.concat([self.df, occupation_dummies], axis=1)\n",
        "        self.df.drop('occupation', axis=1, inplace=True)\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def type_of_loan_handling(self) -> pd.DataFrame:\n",
        "        '''\n",
        "        There are values like \"Credit-Builder Loan, and Mortgage Loan\". Firs of all, this method extracts all\n",
        "        specific type of loans (Credit-Builder Loan,Mortgage Loan etc.). Then creates a columns like; \"has_creadit_builder_loan\".\n",
        "        While creating these columns it checks if that specific loan type exist in the row then return true if exist or false\n",
        "        if not. Finally, true and false values return zeros and ones and original column deleted.\n",
        "        '''\n",
        "\n",
        "        if 'type_of_loan' not in self.df.columns:\n",
        "            raise ValueError ('type_of_loan not found in the DataFrame.')\n",
        "\n",
        "        loan_type_split = self.df['type_of_loan'].str.split(r', and |, | and |,').dropna()\n",
        "\n",
        "        # Flatten the resulting list of lists\n",
        "        loan_types_list = [item.strip() for sublist in loan_type_split.tolist() for item in sublist]\n",
        "\n",
        "        # Geting the unique loan types\n",
        "        unique_loan_types = set(loan_types_list)\n",
        "\n",
        "\n",
        "        for loan_type in unique_loan_types:\n",
        "            if pd.notna(loan_type):\n",
        "                # Creating new columns without spaces but instead with \"_\"\n",
        "                new_column = (loan_type.replace(\" \", \"_\")).lower()\n",
        "\n",
        "                # This line; created columns like \"has_student_loan\" and return true or false, if it's emtpy return pd.NA\n",
        "                self.df[f'has_{new_column}'] = self.df['type_of_loan'].apply(lambda x: loan_type in x if pd.notna(x) else pd.NA)\n",
        "                #This line; transforms true and false values into zeros and ones\n",
        "                self.df[f'has_{new_column}'] = self.df[f'has_{new_column}'].apply(lambda x: int(x) if x is not pd.NA else x)\n",
        "\n",
        "                # Filling null values with mode. Since these columns are binary, mode imputation has used to fill null values.\n",
        "                mode_value = self.df[f'has_{new_column}'].mode().iloc[0]\n",
        "                self.df[f'has_{new_column}'].fillna(mode_value, inplace=True)\n",
        "\n",
        "        self.df.drop('type_of_loan', axis=1, inplace=True)\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def credit_mix_handling(self) -> pd.DataFrame:\n",
        "        if 'credit_mix' not in self.df.columns:\n",
        "            raise ValueError ('\"credit_mix\" not found in the DataFrame.')\n",
        "\n",
        "        # Deleting strange values like \"_\"\n",
        "        self.df['credit_mix'] = self.df['credit_mix'].replace(\"_\", pd.NA)\n",
        "\n",
        "        # Replacing missing values in each row, associated with a specific customer_id, with the most frequent value.\n",
        "        self.df['credit_mix']= self.df.groupby('customer_id')['credit_mix'].transform(lambda x: x.mode()[0])\n",
        "        self.df['credit_mix'].astype(str)\n",
        "\n",
        "        # Binary encoding\n",
        "        credit_mix_dummies = pd.get_dummies(self.df['credit_mix'], drop_first=True, prefix='credit_mix', dtype=int)\n",
        "        self.df = pd.concat([self.df, credit_mix_dummies], axis=1)\n",
        "        self.df.drop('credit_mix', axis=1, inplace=True)\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def payment_of_min_amount_handling(self) -> pd.DataFrame:\n",
        "        if 'payment_of_min_amount' not in self.df.columns:\n",
        "            raise ValueError ('\"payment_of_min_amount\" not found in the DataFrame.')\n",
        "\n",
        "        # Deleting strange values like \"NM\"\n",
        "        self.df['payment_of_min_amount'] = self.df['payment_of_min_amount'].replace(\"NM\", pd.NA)\n",
        "\n",
        "        # Replacing missing values in each row, associated with a specific customer_id, with the most frequent value.\n",
        "        self.df['payment_of_min_amount']= self.df.groupby('customer_id')['payment_of_min_amount'].transform(lambda x: x.mode()[0])\n",
        "        self.df['payment_of_min_amount'].astype(str)\n",
        "\n",
        "        # Binary encoding\n",
        "        payment_of_min_amount_dummies = pd.get_dummies(self.df['payment_of_min_amount'],drop_first=True ,prefix='payment_of_min_amount', dtype=int)\n",
        "        self.df = pd.concat([self.df, payment_of_min_amount_dummies], axis=1)\n",
        "        self.df.drop('payment_of_min_amount', axis=1, inplace=True)\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def payment_behaviour_handling(self) -> pd.DataFrame:\n",
        "        if 'payment_behaviour' not in self.df.columns:\n",
        "            raise ValueError ('\"payment_behaviour\" not found in the DataFrame.')\n",
        "\n",
        "        # Deleting strange values like \"!@9#%8 \"\n",
        "        self.df['payment_behaviour'] = self.df['payment_behaviour'].replace(\"!@9#%8\", pd.NA)\n",
        "\n",
        "        # Replacing missing values in each row, associated with a specific customer_id, with the most frequent value.\n",
        "        self.df['payment_behaviour']= self.df.groupby('customer_id')['payment_behaviour'].transform(lambda x: x.mode()[0])\n",
        "        self.df['payment_behaviour'].astype(str)\n",
        "\n",
        "        # Binary encoding\n",
        "        payment_behaviour_dummies = pd.get_dummies(self.df['payment_behaviour'],drop_first=True , dtype=int)\n",
        "        self.df = pd.concat([self.df, payment_behaviour_dummies], axis=1)\n",
        "        self.df.drop('payment_behaviour', axis=1, inplace=True)\n",
        "\n",
        "        return self.df\n",
        "\n",
        "\n",
        "    def credit_score_handling(self) -> pd.DataFrame:\n",
        "        if 'credit_score' not in self.df.columns:\n",
        "            raise ValueError ('\"credit_score\" not found in the DataFrame.')\n",
        "\n",
        "        # After executing this method values will be Poor==0, Good==1, Standard==2\n",
        "        mapping = {\"Standard\":2, \"Poor\":0, \"Good\":1}\n",
        "        self.df['credit_score'] = self.df['credit_score'].replace(mapping)\n",
        "\n",
        "        # Since this column is the target variable, I expect it to be located at the end of the dataset.\n",
        "        credit_score_column = self.df['credit_score']\n",
        "        self.df = self.df.drop(columns=['credit_score'])\n",
        "        self.df['credit_score'] = credit_score_column\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    #THIS COLUMN SHOULD RUN IN THE END\n",
        "    def column_deleter(self) -> pd.DataFrame:\n",
        "\n",
        "        # All columns lowered one more time.\n",
        "        self.df.columns = self.df.columns.str.lower()\n",
        "\n",
        "        # Deleting unnecessary features.\n",
        "        columns_to_drop = ['name', 'ssn', 'customer_id', 'id']\n",
        "        self.df.drop(columns=columns_to_drop, inplace=True)\n",
        "\n",
        "\n",
        "        return self.df\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:24:42.103885Z",
          "iopub.execute_input": "2023-10-06T17:24:42.104548Z",
          "iopub.status.idle": "2023-10-06T17:24:42.130377Z",
          "shell.execute_reply.started": "2023-10-06T17:24:42.104514Z",
          "shell.execute_reply": "2023-10-06T17:24:42.129204Z"
        },
        "trusted": true,
        "id": "2xaaza1p_Xqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Numeric Features Handling Plan\n",
        "\n",
        "delete all negative numbers\n",
        "\n",
        "### INTEGERS\n",
        "##### age / num_bank_accounts / num_credit_card /  interest_rate / num_of_loan / delay_from_due_date / num_of_delayed_payment / num_credit_inquiries /\n",
        "* There are values like; 24_. Numerics should be parsed from others. // Non number characters should be deleted. // Datapoints should contain only numeric characters.\n",
        "* Replacing missing values in each row, associated with a specific customer_id, with the most frequent value. --> INT\n",
        "\n",
        "##### credit_history_age\n",
        "* From \"2 Years 4 Months\" format to 28 --> INT\n",
        "\n",
        "### FLOATS\n",
        "\n",
        "#####  annual_income / monthly_inhand_salary / changed_credit_limit( _ ) / outstanding_debt / total_emi_per_month /\n",
        "* Non number characters should be deleted. // Datapoints should contain only numeric characters.\n",
        "* Replacing missing values in each row, associated with a specific customer_id, with the most frequent value. --> FLOAT .4\n",
        "\n",
        "##### amount_invested_monthly\t/ monthly_balance\n",
        "* `__10000__` and `__-333333333333333333333333333__` values will be deleted if there any.\n",
        "* Non number characters should be deleted. // Datapoints should contain only numeric characters.\n",
        "* Replacing missing values with the mean --> FLOAT .4\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "suBcLzVW_Xqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "integer_columns_to_handle = ['age',\n",
        "                            'num_bank_accounts',\n",
        "                            'num_credit_card',\n",
        "                            'interest_rate',\n",
        "                            'num_of_loan',\n",
        "                            'delay_from_due_date',\n",
        "                            'num_of_delayed_payment',\n",
        "                            'num_credit_inquiries',\n",
        "                            'credit_history_age']\n",
        "\n",
        "float_columns_to_handle = ['credit_utilization_ratio',\n",
        "                           'annual_income',\n",
        "                           'monthly_inhand_salary',\n",
        "                           'changed_credit_limit',\n",
        "                           'outstanding_debt',\n",
        "                           'total_emi_per_month',\n",
        "                           'amount_invested_monthly',\n",
        "                           'monthly_balance']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:24:43.572697Z",
          "iopub.execute_input": "2023-10-06T17:24:43.57307Z",
          "iopub.status.idle": "2023-10-06T17:24:43.579386Z",
          "shell.execute_reply.started": "2023-10-06T17:24:43.573042Z",
          "shell.execute_reply": "2023-10-06T17:24:43.578035Z"
        },
        "trusted": true,
        "id": "KMfrvqUq_Xqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NumericTransformer():\n",
        "    '''\n",
        "    This class handles numeric features in a DataFrame. It provides methods for handling floating-point features,\n",
        "    integer features, and transforming the 'credit_history_age' column.\n",
        "\n",
        "    Parameters:\n",
        "    - input_df (pd.DataFrame): The input DataFrame to be processed.\n",
        "\n",
        "    Methods:\n",
        "    - floats_handling: Handles floating-point features.\n",
        "    - integers_handling: Handles integer features.\n",
        "    - transform_credit_history_age: Transforms the 'credit_history_age' column.\n",
        "\n",
        "    '''\n",
        "    def __init__(self, input_df:pd.DataFrame):\n",
        "        '''\n",
        "        Initializes the NumericTransformer with the input DataFrame.\n",
        "\n",
        "        Parameters:\n",
        "        - input_df (pd.DataFrame): The input DataFrame to be processed.\n",
        "        '''\n",
        "        self.df = input_df\n",
        "        self.df.columns = self.df.columns.str.lower()\n",
        "\n",
        "    def floats_handling(self) -> pd.DataFrame:\n",
        "        '''\n",
        "        Handles floating-point features in the DataFrame.\n",
        "\n",
        "        Returns:\n",
        "        - pd.DataFrame: The DataFrame with processed floating-point features.\n",
        "        '''\n",
        "        columns_to_process = ['credit_utilization_ratio',\n",
        "                           'annual_income',\n",
        "                           'monthly_inhand_salary',\n",
        "                           'changed_credit_limit',\n",
        "                           'outstanding_debt',\n",
        "                           'total_emi_per_month',\n",
        "                           'amount_invested_monthly',\n",
        "                           'monthly_balance']\n",
        "\n",
        "\n",
        "        # First check if the specified columns exist in dataframe\n",
        "        for column in columns_to_process:\n",
        "            if column not in self.df.columns:\n",
        "                raise ValueError(f\"Column '{column}' not found in the DataFrame.\")\n",
        "\n",
        "        # Deleting specific strange values\n",
        "        mapping = {'__-333333333333333333333333333__':pd.NA,\n",
        "                  '__10000__':pd.NA}\n",
        "        self.df.replace(mapping, inplace=True)\n",
        "\n",
        "        # Cleaning the datapoint from non-numeric characters.\n",
        "        pattern = r'[^0-9.]' # Keeping numbers 0 to 9 and dots(\".\") because these are floats.\n",
        "        for column in columns_to_process:\n",
        "            self.df[column] = self.df[column].astype(str) # All columns will be changed as str to impelement replace\n",
        "            self.df[column] = self.df[column].str.replace(pattern, '', regex=True) # All characters gone except 0 to 9 and dot\n",
        "            self.df[column] = self.df[column].replace('', pd.NA) # After deleting characters some rows could be emtpy, they're NA now\n",
        "\n",
        "\n",
        "        ## FILLING MISSING ROWS ##\n",
        "        # Mode imputation will be applied to some features and mean will be applied to others(last 2)\n",
        "\n",
        "        # Imputing columns except first and last three.\n",
        "        for mode_column in columns_to_process[1:-3]:\n",
        "            self.df[mode_column] = self.df.groupby('customer_id')[mode_column].transform(lambda x: x.mode()[0])\n",
        "\n",
        "\n",
        "        ### columns_to_process[5]/'total_emi_per_month' has 0 values so it will be treated separately ###\n",
        "        # Converting float to be able to apply imputations.\n",
        "        self.df['total_emi_per_month']= (self.df['total_emi_per_month'].astype(float)).round(4)\n",
        "\n",
        "        # Implementing mode imputation to non-null values.\n",
        "        self.df['total_emi_per_month'] = self.df.groupby('customer_id')['total_emi_per_month'].transform(lambda x: x.mode()[0] if not x.isnull().all() else pd.NA)\n",
        "\n",
        "        # There are so many zeros and they're not valid. Replacing(deleting) zeros with pd.NA then mean imputing to missing rows.\n",
        "        self.df['total_emi_per_month'] = self.df['total_emi_per_month'].replace(0, pd.NA)\n",
        "        self.df['total_emi_per_month'].fillna(self.df['total_emi_per_month'].median(), inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "        ## Imputation of last two features; amount_invested_monthly, monthly_balance ##\n",
        "        # There will be mean imputation for these two column\n",
        "        # total_emi_per_month =  Skewness: 2.55, Kurtosis: 8.74\n",
        "        # monthly_balance = Skewness: 1.59, Kurtosis: 2.95\n",
        "        for mean_column in columns_to_process[-2:]:\n",
        "            # Converting the column to a numeric data type, handling non-numeric values\n",
        "            self.df[mean_column] = pd.to_numeric(self.df[mean_column], errors='coerce')\n",
        "            # Filling missing values with the mean\n",
        "            self.df[mean_column] = self.df[mean_column].fillna(self.df[mean_column].mean())\n",
        "\n",
        "        # All columns will be float data type.\n",
        "        self.df[columns_to_process] = self.df[columns_to_process].astype(float).round(4)\n",
        "\n",
        "        return self.df\n",
        "\n",
        "\n",
        "    def integers_handling(self) -> pd.DataFrame:\n",
        "        '''\n",
        "        Handles integer features in the DataFrame.\n",
        "\n",
        "        Returns:\n",
        "        - pd.DataFrame: The DataFrame with processed integer features.\n",
        "        '''\n",
        "        features_to_process = ['age',\n",
        "                              'num_credit_card',\n",
        "                              'interest_rate',\n",
        "                              'delay_from_due_date',\n",
        "                              'num_of_delayed_payment',\n",
        "                              'num_of_loan',\n",
        "                              'num_bank_accounts',\n",
        "                              'num_credit_inquiries'\n",
        "                              ]\n",
        "\n",
        "\n",
        "        # First check if the specified columns exist in dataframe\n",
        "        for column in features_to_process:\n",
        "            if column not in self.df.columns:\n",
        "                raise ValueError(f\"Column '{column}' not found in the DataFrame.\")\n",
        "\n",
        "\n",
        "\n",
        "        pattern = r'[^0-9]'\n",
        "        for column in features_to_process:\n",
        "            self.df[column] = self.df[column].astype(str) # All columns will be changed as str to ne able to impelement replace\n",
        "            self.df[column] = self.df[column].str.replace(pattern, '', regex=True) # All characters gone except 0 to 9\n",
        "            self.df[column] = self.df[column].replace('', pd.NA) #After deleting characters some rows could be emtpy, they're NA now\n",
        "\n",
        "            # Replacing missing values in each row, associated with a specific customer_id, with the most frequent value.\n",
        "            self.df[column] = self.df.groupby('customer_id')[column].transform(lambda x: x.mode()[0])\n",
        "            # Tranformed as int.\n",
        "            self.df[column] = self.df[column].astype(int)\n",
        "\n",
        "\n",
        "        #So many invalid zeros, they need to be handled (last three values in features_to_process).\n",
        "        for column in features_to_process[-3:]:\n",
        "\n",
        "            # I dont remember why I implemented this line of code twice but I am gonna keep it :)\n",
        "            self.df[column] = self.df.groupby('customer_id')[column].transform(lambda x: x.mode()[0] if not x.isnull().all() else pd.NA)\n",
        "            # Replacing zeros with pd.NA then implementing mean imputation.\n",
        "            self.df[column] = self.df[column].replace(0, pd.NA)\n",
        "            self.df[column].fillna(self.df[column].median(), inplace=True)\n",
        "            self.df[column] = self.df[column].astype(int)\n",
        "\n",
        "\n",
        "        return self.df\n",
        "\n",
        "\n",
        "\n",
        "    def transform_credit_history_age(self) -> pd.DataFrame:\n",
        "        '''\n",
        "        Transforms the 'credit_history_age' column from textual format to integer format\n",
        "        (e.g., 3 Years and 1 Month to 37)\n",
        "\n",
        "        Returns:\n",
        "        - pd.DataFrame: The DataFrame with the transformed 'credit_history_age' column.\n",
        "        '''\n",
        "        column_name = 'credit_history_age'\n",
        "\n",
        "        # There are few kinds of null types so I replaced all of them with pd.NA\n",
        "        self.df[column_name] = self.df[column_name].fillna(pd.NA)\n",
        "\n",
        "        # Replacing missing values in each row, associated with a specific customer_id, with the most frequent value.\n",
        "        self.df[column_name] = self.df.groupby('customer_id')[column_name].transform(lambda x: x.mode()[0] if not x.isnull().all() else pd.NA)\n",
        "\n",
        "        # Converting the column to string type\n",
        "        self.df[column_name] = self.df[column_name].astype(str)\n",
        "\n",
        "        # Extracting the numeric values using regular expressions\n",
        "        extracted_values = self.df[column_name].str.extract(r'(\\d+) Years and (\\d+) Months')\n",
        "\n",
        "        # Converting the extracted values to integers and perform the calculation.\n",
        "        self.df[column_name] = (extracted_values[0].astype(int) * 12) + extracted_values[1].astype(int)\n",
        "\n",
        "        return self.df\n",
        "\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:24:44.287346Z",
          "iopub.execute_input": "2023-10-06T17:24:44.287752Z",
          "iopub.status.idle": "2023-10-06T17:24:44.310699Z",
          "shell.execute_reply.started": "2023-10-06T17:24:44.28772Z",
          "shell.execute_reply": "2023-10-06T17:24:44.309705Z"
        },
        "trusted": true,
        "id": "1OJWVF-W_Xqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the preprocessor"
      ],
      "metadata": {
        "id": "aYRu3CNW_Xqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Preprocessor():\n",
        "    '''\n",
        "    A utility class to perform transformations on a DataFrame.\n",
        "    '''\n",
        "    def __init__(self, dataframe:pd.DataFrame):\n",
        "        '''\n",
        "        Initializes numeric and categorical transformes.\n",
        "\n",
        "        Paramaters:\n",
        "        - dataframe(pd.DataFrame): The input DataFrame to be transformed\n",
        "        '''\n",
        "\n",
        "        # Creating copy of original dataframe\n",
        "        self.df = dataframe.copy()\n",
        "\n",
        "        # Creating instances of transformers.\n",
        "        self.num_handler = NumericTransformer(input_df=self.df)\n",
        "        self.cat_handler = CategoricalTransformer(input_df=self.df)\n",
        "\n",
        "\n",
        "    def transform(self) -> pd.DataFrame:\n",
        "\n",
        "        # Numeric transformation\n",
        "        self.df = self.num_handler.floats_handling()\n",
        "        self.df = self.num_handler.integers_handling()\n",
        "        self.df = self.num_handler.transform_credit_history_age()\n",
        "\n",
        "        # Categorical transformation\n",
        "        self.df = self.cat_handler.credit_mix_handling()\n",
        "        self.df = self.cat_handler.month_handling()\n",
        "        self.df = self.cat_handler.occupation_handling()\n",
        "        self.df = self.cat_handler.payment_behaviour_handling()\n",
        "        self.df = self.cat_handler.payment_of_min_amount_handling()\n",
        "        self.df = self.cat_handler.type_of_loan_handling()\n",
        "        self.df = self.cat_handler.credit_score_handling()\n",
        "        self.df = self.cat_handler.column_deleter()\n",
        "\n",
        "        return self.df\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:24:45.527803Z",
          "iopub.execute_input": "2023-10-06T17:24:45.528654Z",
          "iopub.status.idle": "2023-10-06T17:24:45.538767Z",
          "shell.execute_reply.started": "2023-10-06T17:24:45.528609Z",
          "shell.execute_reply": "2023-10-06T17:24:45.537461Z"
        },
        "trusted": true,
        "id": "EDDrEnso_Xqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check if there is any problem about transformation"
      ],
      "metadata": {
        "id": "81I0073j_Xqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = Preprocessor(dataframe=df)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:24:46.501083Z",
          "iopub.execute_input": "2023-10-06T17:24:46.501451Z",
          "iopub.status.idle": "2023-10-06T17:24:46.549709Z",
          "shell.execute_reply.started": "2023-10-06T17:24:46.501424Z",
          "shell.execute_reply": "2023-10-06T17:24:46.548503Z"
        },
        "trusted": true,
        "id": "Z_pEDf_5_Xqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_df = preprocessor.transform() # Creating a new transformed dataframe"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:24:47.805465Z",
          "iopub.execute_input": "2023-10-06T17:24:47.806208Z",
          "iopub.status.idle": "2023-10-06T17:25:54.513572Z",
          "shell.execute_reply.started": "2023-10-06T17:24:47.806167Z",
          "shell.execute_reply": "2023-10-06T17:25:54.512544Z"
        },
        "trusted": true,
        "id": "smhxmmuP_Xqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_df.sample(10)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:28:55.926484Z",
          "iopub.execute_input": "2023-10-06T17:28:55.926905Z",
          "iopub.status.idle": "2023-10-06T17:28:55.964356Z",
          "shell.execute_reply.started": "2023-10-06T17:28:55.926873Z",
          "shell.execute_reply": "2023-10-06T17:28:55.96327Z"
        },
        "trusted": true,
        "id": "VbvQcq8o_Xqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_df.info()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:28:57.605133Z",
          "iopub.execute_input": "2023-10-06T17:28:57.605495Z",
          "iopub.status.idle": "2023-10-06T17:28:57.637361Z",
          "shell.execute_reply.started": "2023-10-06T17:28:57.605466Z",
          "shell.execute_reply": "2023-10-06T17:28:57.636046Z"
        },
        "trusted": true,
        "id": "gkyo0mc3_Xqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### It seems there is no problem about null values or data types."
      ],
      "metadata": {
        "id": "RCBpgZ36_Xqw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NUMERIC COLUMNS HISTOGRAMS (transformed but not standardized)"
      ],
      "metadata": {
        "id": "fMGjOHBM_Xqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_columns = integer_columns_to_handle + float_columns_to_handle"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:29:01.539149Z",
          "iopub.execute_input": "2023-10-06T17:29:01.540036Z",
          "iopub.status.idle": "2023-10-06T17:29:01.54504Z",
          "shell.execute_reply.started": "2023-10-06T17:29:01.540004Z",
          "shell.execute_reply": "2023-10-06T17:29:01.543922Z"
        },
        "trusted": true,
        "id": "6kaLpZPt_Xqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as  plt\n",
        "%matplotlib inline\n",
        "from scipy.stats import skew, kurtosis"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:29:01.869744Z",
          "iopub.execute_input": "2023-10-06T17:29:01.870143Z",
          "iopub.status.idle": "2023-10-06T17:29:02.247066Z",
          "shell.execute_reply.started": "2023-10-06T17:29:01.870089Z",
          "shell.execute_reply": "2023-10-06T17:29:02.245631Z"
        },
        "trusted": true,
        "id": "TgtmbsVc_Xqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examining distribution of the numeric columns with histograms\n",
        "\n",
        "n_rows = 5\n",
        "n_cols = 4\n",
        "\n",
        "# Creating subplots\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 12))\n",
        "\n",
        "# Flatten the axes array to iterate over all subplots\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Loop through the features and create histplots\n",
        "for i, feature in enumerate(numeric_columns):\n",
        "    sns.histplot(data=transformed_df, x=feature, bins=30, ax=axes[i])\n",
        "\n",
        "    axes[i].set_title(f'Distribution of {feature}')  # Replace with your title\n",
        "\n",
        "# Hide any remaining empty subplots\n",
        "for j in range(i + 1, n_rows * n_cols):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()  # Ensure proper spacing\n",
        "plt.show()"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-10-06T17:29:04.030285Z",
          "iopub.execute_input": "2023-10-06T17:29:04.030723Z",
          "iopub.status.idle": "2023-10-06T17:29:08.42579Z",
          "shell.execute_reply.started": "2023-10-06T17:29:04.030693Z",
          "shell.execute_reply": "2023-10-06T17:29:08.424373Z"
        },
        "trusted": true,
        "id": "c_yRM9Pt_Xqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Analyzing"
      ],
      "metadata": {
        "id": "uwkNvc3Q_Xqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureAnalyzer():\n",
        "    '''\n",
        "    This class provides methods to analyze numerical columns in a DataFrame, including statistics such as mean,\n",
        "    median, standard deviation, variance, kurtosis, skewness, maximum, minimum, and count of zeros.\n",
        "\n",
        "    Args:\n",
        "        input_df (pd.DataFrame): The DataFrame containing the numerical columns to be analyzed.\n",
        "\n",
        "    Methods:\n",
        "        column_statistics(columns): Calculates statistics for the specified columns.\n",
        "\n",
        "    '''\n",
        "    def __init__(self, input_df):\n",
        "        self.df = input_df\n",
        "\n",
        "    def column_statistics(self, columns):\n",
        "        '''\n",
        "        Calculate statistics for the specified columns.\n",
        "\n",
        "        Args:\n",
        "            columns (list of str): A list of column names to analyze.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: A DataFrame containing statistics for the specified columns.\n",
        "\n",
        "        Example:\n",
        "            analyzer = FeatureAnalyzer(data_frame)\n",
        "            stats = analyzer.column_statistics(['age', 'income'])\n",
        "        '''\n",
        "\n",
        "        invalid_columns = [col for col in columns if col not in self.df.columns]\n",
        "        if invalid_columns:\n",
        "            print(\"Invalid columns:\", invalid_columns)\n",
        "            return\n",
        "\n",
        "        stats_dict = {}\n",
        "\n",
        "        for column in columns:\n",
        "            # Replace non-numeric values with NaN\n",
        "            numeric_values = pd.to_numeric(self.df[column], errors='coerce')\n",
        "\n",
        "            count_of_zeros = (numeric_values == 0).sum()\n",
        "            std = numeric_values.std()\n",
        "            variance = numeric_values.var()\n",
        "            mean = numeric_values.mean()\n",
        "            median = numeric_values.median()\n",
        "            kurt = kurtosis(numeric_values).round(2)\n",
        "            skewness = skew(numeric_values).round(2)\n",
        "            maximum = numeric_values.max()\n",
        "            minumum = numeric_values.min()\n",
        "\n",
        "            stats_dict[column] = {\n",
        "                'Count of Zeros': count_of_zeros,\n",
        "                'Standard Deviation': std,\n",
        "                'Variance': variance,\n",
        "                'Mean': mean,\n",
        "                'Median': median,\n",
        "                'Kurtosis': kurt,\n",
        "                'Skewness': skewness,\n",
        "                'Max Value': maximum,\n",
        "                'Min Value': minumum\n",
        "            }\n",
        "\n",
        "        return pd.DataFrame.from_dict(stats_dict, orient='index')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:29:08.427949Z",
          "iopub.execute_input": "2023-10-06T17:29:08.428284Z",
          "iopub.status.idle": "2023-10-06T17:29:08.436553Z",
          "shell.execute_reply.started": "2023-10-06T17:29:08.428257Z",
          "shell.execute_reply": "2023-10-06T17:29:08.435695Z"
        },
        "trusted": true,
        "id": "iVNI5osG_Xqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Before NumericTransformer implemented."
      ],
      "metadata": {
        "id": "r3k_JIXj_Xqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer1 = FeatureAnalyzer(input_df=df)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:29:08.437823Z",
          "iopub.execute_input": "2023-10-06T17:29:08.438146Z",
          "iopub.status.idle": "2023-10-06T17:29:08.459471Z",
          "shell.execute_reply.started": "2023-10-06T17:29:08.438121Z",
          "shell.execute_reply": "2023-10-06T17:29:08.458485Z"
        },
        "trusted": true,
        "id": "kp7ZeOEz_Xqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer1.column_statistics(numeric_columns)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:29:08.461626Z",
          "iopub.execute_input": "2023-10-06T17:29:08.461919Z",
          "iopub.status.idle": "2023-10-06T17:29:08.957319Z",
          "shell.execute_reply.started": "2023-10-06T17:29:08.461894Z",
          "shell.execute_reply": "2023-10-06T17:29:08.956253Z"
        },
        "trusted": true,
        "id": "ScvZvL6O_Xqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NumericTransformer implemented (no standardization)"
      ],
      "metadata": {
        "id": "JpyZD5KZ_Xq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer2 = FeatureAnalyzer(input_df=transformed_df)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:29:11.669799Z",
          "iopub.execute_input": "2023-10-06T17:29:11.670178Z",
          "iopub.status.idle": "2023-10-06T17:29:11.675401Z",
          "shell.execute_reply.started": "2023-10-06T17:29:11.670148Z",
          "shell.execute_reply": "2023-10-06T17:29:11.67419Z"
        },
        "trusted": true,
        "id": "Uwp-z9oS_Xq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer2.column_statistics(numeric_columns)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:29:12.09652Z",
          "iopub.execute_input": "2023-10-06T17:29:12.096841Z",
          "iopub.status.idle": "2023-10-06T17:29:12.235768Z",
          "shell.execute_reply.started": "2023-10-06T17:29:12.096815Z",
          "shell.execute_reply": "2023-10-06T17:29:12.235016Z"
        },
        "trusted": true,
        "id": "SH_4Zpt6_Xq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### All columns seem fine for me. There are extreme values but let's see the model performance."
      ],
      "metadata": {
        "id": "aYjakBI__Xq6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODELING"
      ],
      "metadata": {
        "id": "mDnXry6J_Xq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
        "#from sklearn.tree import DecisionTreeClassifier\n",
        "#from catboost import CatBoostClassifier\n",
        "#from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:29:14.774407Z",
          "iopub.execute_input": "2023-10-06T17:29:14.774821Z",
          "iopub.status.idle": "2023-10-06T17:29:15.283645Z",
          "shell.execute_reply.started": "2023-10-06T17:29:14.774792Z",
          "shell.execute_reply": "2023-10-06T17:29:15.282647Z"
        },
        "trusted": true,
        "id": "NJaGSBxh_Xq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = transformed_df.drop('credit_score', axis=1)\n",
        "y = transformed_df['credit_score']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:29:16.318537Z",
          "iopub.execute_input": "2023-10-06T17:29:16.318892Z",
          "iopub.status.idle": "2023-10-06T17:29:16.355842Z",
          "shell.execute_reply.started": "2023-10-06T17:29:16.318865Z",
          "shell.execute_reply": "2023-10-06T17:29:16.354543Z"
        },
        "trusted": true,
        "id": "QtANz3Ek_Xq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:29:17.455576Z",
          "iopub.execute_input": "2023-10-06T17:29:17.455983Z",
          "iopub.status.idle": "2023-10-06T17:29:17.55807Z",
          "shell.execute_reply.started": "2023-10-06T17:29:17.455954Z",
          "shell.execute_reply": "2023-10-06T17:29:17.556897Z"
        },
        "trusted": true,
        "id": "L0HuQLQF_Xq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:29:18.695271Z",
          "iopub.execute_input": "2023-10-06T17:29:18.69562Z",
          "iopub.status.idle": "2023-10-06T17:29:18.700634Z",
          "shell.execute_reply.started": "2023-10-06T17:29:18.695592Z",
          "shell.execute_reply": "2023-10-06T17:29:18.699851Z"
        },
        "trusted": true,
        "id": "yYBI6J_f_Xq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[numeric_columns] = scaler.fit_transform(X_train[numeric_columns])\n",
        "X_test[numeric_columns] = scaler.transform(X_test[numeric_columns])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:29:19.527395Z",
          "iopub.execute_input": "2023-10-06T17:29:19.527985Z",
          "iopub.status.idle": "2023-10-06T17:29:19.612865Z",
          "shell.execute_reply.started": "2023-10-06T17:29:19.527954Z",
          "shell.execute_reply": "2023-10-06T17:29:19.611679Z"
        },
        "trusted": true,
        "id": "ibda-hx-_Xq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score,make_scorer, f1_score"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:29:20.726531Z",
          "iopub.execute_input": "2023-10-06T17:29:20.72689Z",
          "iopub.status.idle": "2023-10-06T17:29:20.732018Z",
          "shell.execute_reply.started": "2023-10-06T17:29:20.726863Z",
          "shell.execute_reply": "2023-10-06T17:29:20.730946Z"
        },
        "trusted": true,
        "id": "vIIcRFkb_Xq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"XGBoost\": XGBClassifier()\n",
        "}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:29:21.247319Z",
          "iopub.execute_input": "2023-10-06T17:29:21.247669Z",
          "iopub.status.idle": "2023-10-06T17:29:21.252353Z",
          "shell.execute_reply.started": "2023-10-06T17:29:21.247644Z",
          "shell.execute_reply": "2023-10-06T17:29:21.251314Z"
        },
        "trusted": true,
        "id": "7IWQGX1__Xq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    \"Random Forest\": {\n",
        "        'n_estimators': [16, 32, 64, 128],\n",
        "        'max_depth': [None, 10, 20],\n",
        "       # 'min_samples_split': [5, 10],\n",
        "        #'min_samples_leaf': [2, 5],\n",
        "        #'bootstrap': [True, False],\n",
        "    },\n",
        "    \"XGBoost\": {\n",
        "        'learning_rate': [0.1, 0.05, 0.001],\n",
        "        'n_estimators': [16, 32, 64, 128],\n",
        "        #'max_depth': [3, 4, 5],\n",
        "        'subsample': [0.7, 0.8]\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:29:22.446923Z",
          "iopub.execute_input": "2023-10-06T17:29:22.447328Z",
          "iopub.status.idle": "2023-10-06T17:29:22.453297Z",
          "shell.execute_reply.started": "2023-10-06T17:29:22.447297Z",
          "shell.execute_reply": "2023-10-06T17:29:22.452184Z"
        },
        "trusted": true,
        "id": "IO1RY8K1_Xq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_scorer = make_scorer(f1_score, average='weighted')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:29:24.950198Z",
          "iopub.execute_input": "2023-10-06T17:29:24.951314Z",
          "iopub.status.idle": "2023-10-06T17:29:24.956243Z",
          "shell.execute_reply.started": "2023-10-06T17:29:24.951277Z",
          "shell.execute_reply": "2023-10-06T17:29:24.954954Z"
        },
        "trusted": true,
        "id": "fmJMVJMq_Xq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_searches = {}\n",
        "for model_name, model in models.items():\n",
        "    grid_search = GridSearchCV(\n",
        "        model,\n",
        "        params[model_name],\n",
        "        cv=3,  # Use the number of desired cross-validation folds\n",
        "        scoring=f1_scorer,\n",
        "        n_jobs=-1,  # Use all available CPU cores\n",
        "        verbose=2,\n",
        "    )\n",
        "    grid_searches[model_name] = grid_search\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:29:25.791271Z",
          "iopub.execute_input": "2023-10-06T17:29:25.7917Z",
          "iopub.status.idle": "2023-10-06T17:29:25.798535Z",
          "shell.execute_reply.started": "2023-10-06T17:29:25.791667Z",
          "shell.execute_reply": "2023-10-06T17:29:25.796682Z"
        },
        "trusted": true,
        "id": "p9keFm9d_Xq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_searches"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:29:51.839595Z",
          "iopub.execute_input": "2023-10-06T17:29:51.839994Z",
          "iopub.status.idle": "2023-10-06T17:29:51.856723Z",
          "shell.execute_reply.started": "2023-10-06T17:29:51.839965Z",
          "shell.execute_reply": "2023-10-06T17:29:51.855622Z"
        },
        "trusted": true,
        "id": "Luy3bvLj_Xq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_models = {}\n",
        "for model_name, grid_search in grid_searches.items():\n",
        "    grid_search.fit(X_train, y_train)  # X_train and y_train are your training data\n",
        "    best_models[model_name] = grid_search.best_estimator_\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:31:46.103915Z",
          "iopub.execute_input": "2023-10-06T17:31:46.104804Z",
          "iopub.status.idle": "2023-10-06T17:54:36.961429Z",
          "shell.execute_reply.started": "2023-10-06T17:31:46.104754Z",
          "shell.execute_reply": "2023-10-06T17:54:36.960325Z"
        },
        "trusted": true,
        "id": "0ENqahZu_Xq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_models"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:56:56.021426Z",
          "iopub.execute_input": "2023-10-06T17:56:56.02191Z",
          "iopub.status.idle": "2023-10-06T17:56:56.032398Z",
          "shell.execute_reply.started": "2023-10-06T17:56:56.021864Z",
          "shell.execute_reply": "2023-10-06T17:56:56.031169Z"
        },
        "trusted": true,
        "id": "SMduXKvd_Xq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_f1_score = -1  # Initialize with a low value\n",
        "best_model = None\n",
        "\n",
        "for model_name, grid_search in grid_searches.items():\n",
        "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
        "    print(f\"Best F1-score for {model_name}: {grid_search.best_score_}\")\n",
        "    print(\"==\"*25,\"\\n\")\n",
        "\n",
        "    if grid_search.best_score_ > best_f1_score:\n",
        "        best_f1_score = grid_search.best_score_\n",
        "        best_model = grid_search.best_estimator_\n",
        "\n",
        "if best_model is not None:\n",
        "    print(\"Best model based on F1-score:\")\n",
        "    print(best_model)\n",
        "    print(f\"Best F1-score: {best_f1_score}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:57:03.127171Z",
          "iopub.execute_input": "2023-10-06T17:57:03.127812Z",
          "iopub.status.idle": "2023-10-06T17:57:03.134423Z",
          "shell.execute_reply.started": "2023-10-06T17:57:03.127781Z",
          "shell.execute_reply": "2023-10-06T17:57:03.133508Z"
        },
        "trusted": true,
        "id": "XZRa2Ku-_Xq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_model)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:57:09.979751Z",
          "iopub.execute_input": "2023-10-06T17:57:09.980419Z",
          "iopub.status.idle": "2023-10-06T17:57:09.985524Z",
          "shell.execute_reply.started": "2023-10-06T17:57:09.980388Z",
          "shell.execute_reply": "2023-10-06T17:57:09.98442Z"
        },
        "trusted": true,
        "id": "zc4u-kGK_Xq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:57:28.87243Z",
          "iopub.execute_input": "2023-10-06T17:57:28.872825Z",
          "iopub.status.idle": "2023-10-06T17:57:28.877637Z",
          "shell.execute_reply.started": "2023-10-06T17:57:28.872795Z",
          "shell.execute_reply": "2023-10-06T17:57:28.876849Z"
        },
        "trusted": true,
        "id": "zVBb-Ozn_Xq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = best_model.predict(X_test)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Plot a heatmap for the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title(f'Confusion Matrix for {best_model}')\n",
        "plt.show()\n",
        "\n",
        "# Print the classification report\n",
        "print(f\"Classification Report for {best_model}:\\n\")\n",
        "print(report)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-06T17:59:34.007412Z",
          "iopub.execute_input": "2023-10-06T17:59:34.00777Z",
          "iopub.status.idle": "2023-10-06T17:59:35.194494Z",
          "shell.execute_reply.started": "2023-10-06T17:59:34.007743Z",
          "shell.execute_reply": "2023-10-06T17:59:35.193457Z"
        },
        "trusted": true,
        "id": "BKzkRVpT_Xq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This confusion matrix means that;\n",
        "\n",
        "### Class 0 (Poor):\n",
        "* Actual Count: 5874\n",
        "* Correctly Predicted as Poor: 4973\n",
        "* Incorrectly Predicted as Good: 4\n",
        "* Incorrectly Predicted as Standard: 897\n",
        "\n",
        "### Class 1 (Good):\n",
        "* Actual Count: 3527\n",
        "* Correctly Predicted as Good: 2689\n",
        "* Incorrectly Predicted as Poor: 3\n",
        "* Incorrectly Predicted as Standard: 835\n",
        "\n",
        "### Class 2 (Standard):\n",
        "* Actual Count: 10599\n",
        "* Correctly Predicted as Standard: 8658\n",
        "* Incorrectly Predicted as Poor: 1243\n",
        "* Incorrectly Predicted as Good: 698"
      ],
      "metadata": {
        "id": "1ZqzelXW_Xq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "FOR THOSE WHO WANT TO FIND OUT IF THERE IS ANY CHANCE TO HAVE BETTER OPTIONS.\n",
        "IF YOU WANT TO EXPLORE MORE JUST REPLACE models and params WITH REAL ONES.\n",
        "'''\n",
        "\n",
        "models = {\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
        "    \"XGBoost\": XGBClassifier(),\n",
        "    \"CatBoost\": CatBoostClassifier(verbose=False),\n",
        "    \"AdaBoost\": AdaBoostClassifier(),\n",
        "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
        "}\n",
        "\n",
        "\n",
        "params = {\n",
        "    \"Random Forest\": {\n",
        "        'n_estimators': [8, 16, 32, 64, 128, 256],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'bootstrap': [True, False],\n",
        "    },\n",
        "    \"Decision Tree\": {\n",
        "        'criterion': ['gini', 'entropy'],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "    },\n",
        "    \"Gradient Boosting\": {\n",
        "        'learning_rate': [0.1, 0.01, 0.001],\n",
        "        'n_estimators': [8, 16, 32, 64, 128, 256],\n",
        "        'max_depth': [3, 4, 5],\n",
        "        'subsample': [0.6, 0.7, 0.8, 0.9],\n",
        "    },\n",
        "    \"XGBoost\": {\n",
        "        'learning_rate': [0.1, 0.01, 0.05, 0.001],\n",
        "        'n_estimators': [8, 16, 32, 64, 128, 256],\n",
        "        'max_depth': [3, 4, 5],\n",
        "        'subsample': [0.6, 0.7, 0.8, 0.9],\n",
        "    },\n",
        "    \"CatBoost\": {\n",
        "        'depth': [6, 8, 10],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'iterations': [30, 50, 100],\n",
        "    },\n",
        "    \"AdaBoost\": {\n",
        "        'learning_rate': [0.1, 0.01, 0.5, 0.001],\n",
        "        'n_estimators': [8, 16, 32, 64, 128, 256],\n",
        "    },\n",
        "    \"K-Nearest Neighbors\": {\n",
        "        'n_neighbors': [3, 5, 7, 9],\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'p': [1, 2],\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-03T18:52:35.67453Z",
          "iopub.status.idle": "2023-10-03T18:52:35.674893Z",
          "shell.execute_reply.started": "2023-10-03T18:52:35.674732Z",
          "shell.execute_reply": "2023-10-03T18:52:35.674748Z"
        },
        "trusted": true,
        "id": "xUShwt4j_Xq9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}